# LLM Cost Tracking & Analytics System - Feature Specification

**Status**: ‚úÖ Implementation Complete | üìä Awaiting Data Collection (30+ days)
**Version**: 2.1.0
**Date Created**: 2026-01-03
**Last Updated**: 2026-01-05
**Priority**: Critical (Business Intelligence + Content Safety)
**Assigned To**: Agent Planner + Agent Coder
**GitHub Issue**: TBD

---

## Overview

Criar sistema abrangente de rastreamento, mensura√ß√£o e an√°lise de custos de LLM (Large Language Models) para:
1. Calcular custo real por feature
2. Otimizar pricing de cr√©ditos
3. Avaliar lucratividade por plano (FREE, PLUS, PREMIUM)
4. Identificar oportunidades de otimiza√ß√£o
5. Planejar sustentabilidade financeira
6. **[NEW]** Classificar conte√∫do e validar idade de usu√°rios
7. **[NEW]** Rotear requisi√ß√µes para modelos otimizados por feature

**Problema Atual**:
- N√£o sabemos quanto gastamos em LLM por feature
- Pre√ßo de cr√©ditos n√£o √© baseado em dados reais
- Imposs√≠vel calcular lucratividade de planos pagos
- N√£o h√° visibility de custos operacionais
- Decis√µes de pricing s√£o baseadas em estimativas
- **[NEW]** Conte√∫do NSFW n√£o √© filtrado por idade do usu√°rio
- **[NEW]** Modelo fixo sem otimiza√ß√£o de custo/qualidade

**Solu√ß√£o Proposta**:
- Sistema de tracking de tokens (input/output)
- C√°lculo autom√°tico de custos por requisi√ß√£o
- Dashboard de analytics de custos
- Recomenda√ß√µes de pricing baseadas em dados
- ROI analysis por feature e por plano
- **[NEW]** Classifica√ß√£o de conte√∫do via LLM (Gemini 2.5 Flash-Lite)
- **[NEW]** Valida√ß√£o de idade do usu√°rio vs classifica√ß√£o de conte√∫do
- **[NEW]** Router de modelos otimizado por feature (Venice AI FREE para chat, Grok 4-1 para gera√ß√£o)

---

## Unplanned Additions (Implemented During Development)

Durante a implementa√ß√£o, foram identificadas necessidades cr√≠ticas que resultaram em adi√ß√µes ao escopo original:

### 1. Content Classification System

**Problema Identificado**: Conte√∫do NSFW estava sendo gerado sem controle, sem valida√ß√£o por idade do usu√°rio.

**Solu√ß√£o Implementada**:
- Arquivo: `backend/src/services/contentClassification/index.ts`
- LLM: Gemini 2.5 Flash-Lite (r√°pido, barato, multilingual)
- Classifica conte√∫do em: L, TEN, TWELVE, FOURTEEN, SIXTEEN, EIGHTEEN
- Determina SFW/NSFW
- Valida idade do usu√°rio vs classifica√ß√£o et√°ria

**Impacto**:
- ‚úÖ Chat: Mensagens s√£o classificadas antes de gerar resposta
- ‚úÖ Character Generation: Descri√ß√µes s√£o validadas antes de gerar
- ‚úÖ Character Edit: Edi√ß√µes s√£o validadas antes de salvar
- Usu√°rio < idade m√≠nima recebe erro 403 com mensagem clara

### 2. Model Router Service

**Problema Identificado**: C√≥digo usava modelo fixo sem considerar custo/qualidade por feature.

**Solu√ß√£o Implementada**:
- Arquivo: `backend/src/services/llm/modelRouter.ts`
- Router baseado em feature type (n√£o em conte√∫do)
- Estrat√©gia de roteamento otimizada:

| Feature | Provider | Model | Motivo |
|---------|----------|-------|--------|
| CHAT | OpenRouter | Venice AI (Dolphin Mistral 24B) | FREE, NSFW-friendly |
| CHARACTER_GENERATION | Grok | Grok 4-1 Fast Reasoning | NSFW-friendly, custo-efetivo |
| STORY_GENERATION | Grok | Grok 4-1 Fast Reasoning | NSFW-friendly |
| IMAGE_ANALYSIS | Grok | Grok 4-1 Fast Non-Reasoning | Melhor vis√£o, baixa censura |
| CONTENT_TRANSLATION | Grok | Grok 4-1 Fast Non-Reasoning | NSFW-friendly |
| SYSTEM_TRANSLATION (SFW) | Gemini | Gemini 3 Flash Preview | R√°pido |
| SYSTEM_TRANSLATION (NSFW) | Grok | Grok 4-1 Fast Non-Reasoning | NSFW-friendly |

### 3. OpenRouter Integration

**Problema Identificado**: Necessidade de modelo gratuito para chat (alto volume).

**Solu√ß√£o Implementada**:
- Arquivo: `backend/src/services/llm/openrouter.ts`
- Integra√ß√£o via API OpenAI-compatible
- Modelo: `cognitivecomputations/dolphin-mistral-24b-venice-edition:free`
- Custo: $0 para chat (modelo gratuito da Venice AI)
- Valida√ß√£o de modelo desabilitada para OpenRouter (modelos din√¢micos)

### 4. Message Decryption Handling

**Problema Identificado**: Mensagens do chat s√£o criptografadas no banco de dados, impedindo classifica√ß√£o de conte√∫do.

**Solu√ß√£o Implementada**:
- Arquivo: `backend/src/agents/responseGenerationAgent.ts`
- Fun√ß√£o `isEncrypted()` detecta formato criptografado
- Fun√ß√£o `decryptMessage()` descriptografa antes da classifica√ß√£o
- Classifica√ß√£o agora analisa texto original, n√£o criptografado

### 5. Error Propagation for Age Validation

**Problema Identificado**: Erros de valida√ß√£o de idade eram capturados e transformados em fallback silencioso.

**Solu√ß√£o Implementada**:
- Modifica√ß√£o em: `automatedCharacterGenerationController.ts`, `responseQueue.ts`
- Erros de valida√ß√£o s√£o propagados ao usu√°rio
- Chat: Sistema envia mensagem via WebSocket com aviso
- Character Gen: Retorna erro 403 com mensagem clara

**Files Modified**:
- `backend/src/services/contentClassification/index.ts` (novo)
- `backend/src/services/llm/modelRouter.ts` (novo)
- `backend/src/services/llm/openrouter.ts` (novo)
- `backend/src/agents/responseGenerationAgent.ts` (modificado)
- `backend/src/controllers/automatedCharacterGenerationController.ts` (modificado)
- `backend/src/queues/responseQueue.ts` (modificado)
- `backend/src/routes/v1/characters.ts` (modificado)

---

## Business Value

### Problemas Cr√≠ticos

**1. Falta de Visibility de Custos**:
- N√£o sabemos custo real de gera√ß√£o de personagens
- N√£o sabemos custo de tradu√ß√£o autom√°tica
- N√£o sabemos quanto gastamos com usu√°rios gratuitos
- Imposs√≠vel prever custos mensais com crescimento

**2. Pricing Arbitr√°rio**:
- Cr√©ditos: Quanto vale 1 cr√©dito em $USD?
- Plano PLUS ($5/m√™s): Quantos cr√©ditos devemos dar?
- Plano PREMIUM ($10/m√™s): Quantos cr√©ditos s√£o sustent√°veis?
- Daily credits FREE: Quanto podemos dar sem preju√≠zo?

**3. Risco de Preju√≠zo**:
- Usu√°rio PLUS paga $5 mas pode gastar $10 em LLM
- Usu√°rios gratuitos podem custar mais que geram valor
- Features podem ter custo > receita

**Impactos**:
- üí∏ **Risco Financeiro**: Preju√≠zo n√£o monitorado
- üìä **Decis√µes Cegas**: Sem dados para pricing
- ‚ö†Ô∏è **Insustentabilidade**: Modelo de neg√≥cio pode ser invi√°vel
- üéØ **Falta de Otimiza√ß√£o**: N√£o sabemos onde economizar

---

## Business Objectives

### Primary Goals

1. **Cost Visibility**: Saber exatamente quanto gastamos em LLM
2. **Data-Driven Pricing**: Definir pre√ßo de cr√©ditos baseado em custos reais
3. **Profitability Analysis**: Calcular lucro por plano (FREE, PLUS, PREMIUM)
4. **Optimization Opportunities**: Identificar features caras e alternativas
5. **Financial Sustainability**: Garantir modelo de neg√≥cio vi√°vel

### Success Metrics

**Phase 1 - Tracking (Foundation)**:
- [x] 100% das requisi√ß√µes LLM rastreadas
- [x] Custo calculado em tempo real
- [ ] Database com hist√≥rico de 30+ dias

**Phase 1.5 - Content Filtering (Unplanned Addition)**:
- [x] Content classification service implementado
- [x] Age validation implementado
- [x] Model router implementado
- [x] OpenRouter integration (Venice AI FREE)
- [x] Chat filtering ativo
- [x] Character generation filtering ativo
- [x] Character edit filtering ativo

**Phase 2 - Analytics & Dashboard (COMPLETED)**:
- [x] Dashboard com custos por feature
- [x] Custo m√©dio por usu√°rio (FREE vs PAID)
- [x] ROI por plano calculado
- [x] Internacionaliza√ß√£o completa (12 idiomas)

**Phase 3 - Optimization (Action - PENDING, awaiting 30+ days of data)**:
- [ ] Identificar top 3 features mais caras
- [ ] Recomendar pricing √≥timo de cr√©ditos
- [ ] Plano de redu√ß√£o de custos (target: -20% sem perder qualidade)

---

## Technical Implementation

### Phase 1: LLM Models Inventory & Cost Research

#### Step 1.1: Map All LLM Models Used

**Current LLM Providers** (based on codebase analysis):
- Google Gemini (via Google Generative AI SDK)
- OpenAI (GPT models)
- XAI Grok (via HTTP API)
- Anthropic Claude (via API) - if used

**Action Items**:
```bash
# Find all LLM API calls in codebase
grep -r "gemini" backend/src --include="*.ts"
grep -r "openai" backend/src --include="*.ts"
grep -r "grok" backend/src --include="*.ts"
grep -r "claude" backend/src --include="*.ts"
```

**Expected Models**:
- Gemini 1.5 Pro (for complex analysis, character generation)
- Gemini 1.5 Flash (for fast, cheaper tasks)
- Gemini Vision (for image analysis - Civit.ai)
- GPT-4 Turbo (if used for critical tasks)
- GPT-3.5 Turbo (if used for cheaper tasks)

---

#### Step 1.2: Research Current Pricing (as of January 2026)

**Action**: Web research for latest pricing

**Google Gemini Pricing** (2026-01 - VERIFY):
```
Gemini 1.5 Pro:
  - Input: $0.00025 / 1K tokens ($0.25 / 1M tokens)
  - Output: $0.001 / 1K tokens ($1.00 / 1M tokens)
  - Context: 2M tokens

Gemini 1.5 Flash:
  - Input: $0.000075 / 1K tokens ($0.075 / 1M tokens)
  - Output: $0.0003 / 1K tokens ($0.30 / 1M tokens)
  - Context: 1M tokens

Gemini 1.5 Flash-8B (cheapest):
  - Input: $0.0000375 / 1K tokens ($0.0375 / 1M tokens)
  - Output: $0.00015 / 1K tokens ($0.15 / 1M tokens)
```

**OpenAI Pricing** (2026-01 - VERIFY):
```
GPT-4 Turbo:
  - Input: $0.01 / 1K tokens ($10 / 1M tokens)
  - Output: $0.03 / 1K tokens ($30 / 1M tokens)

GPT-4o:
  - Input: $0.005 / 1K tokens ($5 / 1M tokens)
  - Output: $0.015 / 1K tokens ($15 / 1M tokens)

GPT-4o-mini:
  - Input: $0.00015 / 1K tokens ($0.15 / 1M tokens)
  - Output: $0.0006 / 1K tokens ($0.60 / 1M tokens)

GPT-3.5 Turbo:
  - Input: $0.0005 / 1K tokens ($0.50 / 1M tokens)
  - Output: $0.0015 / 1K tokens ($1.50 / 1M tokens)
```

**Alternative Providers** (Research for NSFW-friendly):
```
Anthropic Claude 3.5 Sonnet:
  - Input: $0.003 / 1K tokens ($3 / 1M tokens)
  - Output: $0.015 / 1K tokens ($15 / 1M tokens)

Together.ai (open models):
  - Llama 3 70B: $0.0009 / 1M tokens (input+output avg)
  - Mixtral 8x7B: $0.0006 / 1M tokens
  - NSFW-friendly models available

Groq (ultra-fast inference):
  - Llama 3 70B: $0.00059 / 1M tokens (input)
  - Mixtral 8x7B: $0.00024 / 1M tokens
```

---

### Phase 2: Map Features to LLM Usage

**File**: `docs/05-business/analysis/llm-cost-mapping.md` (to be created)

#### Feature 1: Character Generation (AI)

**LLM Calls**:
1. **Image Analysis** (if user uploads image)
   - Model: Gemini 1.5 Flash (with Vision)
   - Input: ~500 tokens (image description prompt)
   - Output: ~300 tokens (gender, species, physical traits)
   - Cost: ~$0.0003/call

2. **Character Data Compilation**
   - Model: Gemini 1.5 Pro
   - Input: ~800 tokens (user description + image analysis + template)
   - Output: ~600 tokens (full character JSON)
   - Cost: ~$0.0008/call

**Total Cost per Character Generation**: ~$0.0011 (with image) or ~$0.0008 (text only)

**Current Credit Price**: 75-100 credits
**Optimal Credit Price**: TBD (after full analysis)

---

#### Feature 2: Story Generation (AI)

**LLM Calls**:
1. **Image Analysis** (if user uploads cover image)
   - Model: Gemini 1.5 Flash (Vision)
   - Input: ~400 tokens
   - Output: ~250 tokens
   - Cost: ~$0.00025/call

2. **Story Compilation**
   - Model: Gemini 1.5 Pro
   - Input: ~1200 tokens (description + characters + plot template)
   - Output: ~1000 tokens (title, synopsis, opening, objectives)
   - Cost: ~$0.0013/call

**Total Cost per Story Generation**: ~$0.00155 (with image) or ~$0.0013 (text only)

**Current Credit Price**: 75-100 credits

---

#### Feature 3: Content Translation

**LLM Calls**:
1. **Character Translation**
   - Model: Gemini 1.5 Flash
   - Input: ~600 tokens (original content + target language)
   - Output: ~600 tokens (translated content)
   - Cost: ~$0.00033/call

2. **Story Translation**
   - Model: Gemini 1.5 Flash
   - Input: ~1500 tokens (story content + target language)
   - Output: ~1500 tokens (translated)
   - Cost: ~$0.00068/call

**Caching**: After first translation, cached for 90% cheaper
**Cost per Translation**: ~$0.00033 (first time), ~$0.00003 (cached)

---

#### Feature 4: Chat Messages (Roleplay)

**LLM Calls**:
1. **Message Generation**
   - Model: Gemini 1.5 Flash (fast response)
   - Input: ~800 tokens (character + conversation history + user message)
   - Output: ~200 tokens (AI response)
   - Cost: ~$0.00015/message

**Frequency**: High volume (users send many messages)
**Cost per User per Day**: ~$0.015 (100 messages average)

---

#### Feature 5: Automated Character Generation (Civit.ai)

**LLM Calls**:
1. **Image Analysis** (for curation)
   - Model: Gemini 1.5 Flash (Vision)
   - Input: ~400 tokens
   - Output: ~300 tokens
   - Cost: ~$0.0003/image

2. **Character Compilation**
   - Model: Gemini 1.5 Pro
   - Input: ~700 tokens
   - Output: ~500 tokens
   - Cost: ~$0.0007/character

**Daily Volume**: 20 characters/day
**Daily Cost**: ~$0.02/day = ~$0.60/month

**Note**: This is automated (no user credits), pure operational cost

---

### Phase 3: Database Schema for Tracking

**File**: `prisma/schema.prisma`

```prisma
// LLM Cost Tracking
model LLMUsageLog {
  id          String   @id @default(cuid())

  // Context
  userId      String?
  user        User?    @relation(fields: [userId], references: [id])
  feature     FeatureType // CHARACTER_GEN, STORY_GEN, TRANSLATION, CHAT, etc
  featureId   String?  // ID of character, story, etc (for drill-down)

  // LLM Details
  provider    LLMProvider // GEMINI, OPENAI, XAI, etc
  model       String   // e.g., "gemini-1.5-pro", "gpt-4o-mini"
  operation   String?  // e.g., "character_analysis", "translation"

  // Token Usage
  inputTokens  Int
  outputTokens Int
  totalTokens  Int

  // Cost Calculation
  inputCost    Decimal  @db.Decimal(10, 6) // Cost in USD
  outputCost   Decimal  @db.Decimal(10, 6)
  totalCost    Decimal  @db.Decimal(10, 6)

  // Timing
  latency      Int?     // Response time in ms
  cached       Boolean  @default(false) // Was response cached?

  // Metadata
  metadata     Json?    // Additional context

  createdAt    DateTime @default(now())

  @@index([userId, feature, createdAt])
  @@index([feature, createdAt])
  @@index([createdAt])
}

enum FeatureType {
  CHARACTER_GENERATION
  STORY_GENERATION
  CONTENT_TRANSLATION
  CHAT_MESSAGE
  AUTOMATED_GENERATION
  IMAGE_ANALYSIS
  OTHER
}

enum LLMProvider {
  GEMINI
  OPENAI
  XAI
  ANTHROPIC
  TOGETHER_AI
  GROQ
}

// LLM Pricing Configuration
model LLMPricing {
  id          String   @id @default(cuid())

  provider    LLMProvider
  model       String   // e.g., "gemini-1.5-pro"

  // Pricing per 1M tokens (USD)
  inputPricePerMillion  Decimal @db.Decimal(10, 4)
  outputPricePerMillion Decimal @db.Decimal(10, 4)

  // Effective date
  effectiveFrom DateTime @default(now())
  effectiveTo   DateTime?

  // Metadata
  source      String?  // e.g., "https://ai.google.dev/pricing"
  notes       String?

  isActive    Boolean  @default(true)
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  @@unique([provider, model, effectiveFrom])
}
```

---

### Phase 4: Tracking Service

**File**: `backend/src/services/llm/llmUsageTracker.ts`

```typescript
import { PrismaClient, LLMProvider, FeatureType } from '@prisma/client';

const prisma = new PrismaClient();

export interface LLMUsageParams {
  userId?: string;
  feature: FeatureType;
  featureId?: string;
  provider: LLMProvider;
  model: string;
  inputTokens: number;
  outputTokens: number;
  latency?: number;
  cached?: boolean;
  metadata?: any;
}

export async function trackLLMUsage(params: LLMUsageParams) {
  const {
    userId,
    feature,
    featureId,
    provider,
    model,
    inputTokens,
    outputTokens,
    latency,
    cached = false,
    metadata
  } = params;

  // 1. Get pricing for this model
  const pricing = await getLLMPricing(provider, model);

  if (!pricing) {
    console.warn(`No pricing found for ${provider}:${model}, skipping tracking`);
    return null;
  }

  // 2. Calculate costs
  const inputCost = (inputTokens / 1_000_000) * Number(pricing.inputPricePerMillion);
  const outputCost = (outputTokens / 1_000_000) * Number(pricing.outputPricePerMillion);
  const totalCost = inputCost + outputCost;

  // 3. If cached, reduce cost by 90%
  const finalCost = cached ? totalCost * 0.1 : totalCost;

  // 4. Log to database
  const log = await prisma.lLMUsageLog.create({
    data: {
      userId,
      feature,
      featureId,
      provider,
      model,
      inputTokens,
      outputTokens,
      totalTokens: inputTokens + outputTokens,
      inputCost: cached ? inputCost * 0.1 : inputCost,
      outputCost: cached ? outputCost * 0.1 : outputCost,
      totalCost: finalCost,
      latency,
      cached,
      metadata
    }
  });

  return log;
}

async function getLLMPricing(provider: LLMProvider, model: string) {
  return prisma.lLMPricing.findFirst({
    where: {
      provider,
      model,
      isActive: true,
      effectiveFrom: { lte: new Date() },
      OR: [
        { effectiveTo: null },
        { effectiveTo: { gte: new Date() } }
      ]
    },
    orderBy: { effectiveFrom: 'desc' }
  });
}

// Helper: Wrap LLM calls with automatic tracking
export async function withTracking<T>(
  llmCall: () => Promise<{ result: T; usage: { inputTokens: number; outputTokens: number } }>,
  params: Omit<LLMUsageParams, 'inputTokens' | 'outputTokens' | 'latency'>
): Promise<T> {
  const startTime = Date.now();

  const { result, usage } = await llmCall();

  const latency = Date.now() - startTime;

  await trackLLMUsage({
    ...params,
    inputTokens: usage.inputTokens,
    outputTokens: usage.outputTokens,
    latency
  });

  return result;
}
```

---

### Phase 5: Integration with Existing Services

**Example: Character Generation**

**File**: `backend/src/services/characterGeneration/characterGenerationService.ts`

```typescript
import { trackLLMUsage } from '../llm/llmUsageTracker';

export async function generateCharacter(params: GenerateCharacterParams) {
  // ... existing code

  // Wrap LLM call with tracking
  const analysisResult = await geminiClient.analyzeImage(imageUrl);

  // Track usage
  await trackLLMUsage({
    userId: params.userId,
    feature: 'CHARACTER_GENERATION',
    featureId: null, // Character not created yet
    provider: 'GEMINI',
    model: 'gemini-1.5-flash',
    inputTokens: analysisResult.usage.inputTokens,
    outputTokens: analysisResult.usage.outputTokens,
    metadata: { step: 'image_analysis' }
  });

  // ... continue with character compilation

  const compilationResult = await geminiClient.compileCharacter(analysisData);

  await trackLLMUsage({
    userId: params.userId,
    feature: 'CHARACTER_GENERATION',
    featureId: character.id, // Now we have character ID
    provider: 'GEMINI',
    model: 'gemini-1.5-pro',
    inputTokens: compilationResult.usage.inputTokens,
    outputTokens: compilationResult.usage.outputTokens,
    metadata: { step: 'character_compilation' }
  });

  return character;
}
```

---

### Phase 6: Analytics & Reporting

**File**: `backend/src/services/analytics/llmCostAnalytics.ts`

```typescript
export async function getCostByFeature(dateRange: { from: Date; to: Date }) {
  const costs = await prisma.lLMUsageLog.groupBy({
    by: ['feature'],
    where: {
      createdAt: {
        gte: dateRange.from,
        lte: dateRange.to
      }
    },
    _sum: {
      totalCost: true,
      totalTokens: true
    },
    _count: {
      id: true
    }
  });

  return costs.map(c => ({
    feature: c.feature,
    totalCost: Number(c._sum.totalCost || 0),
    totalTokens: c._sum.totalTokens || 0,
    requestCount: c._count.id,
    avgCostPerRequest: Number(c._sum.totalCost || 0) / c._count.id
  }));
}

export async function getCostByUser(userId: string, dateRange: { from: Date; to: Date }) {
  const costs = await prisma.lLMUsageLog.aggregate({
    where: {
      userId,
      createdAt: {
        gte: dateRange.from,
        lte: dateRange.to
      }
    },
    _sum: {
      totalCost: true
    }
  });

  return {
    userId,
    totalCost: Number(costs._sum.totalCost || 0),
    dateRange
  };
}

export async function getAverageCostByPlan() {
  // Get average LLM cost per user grouped by subscription plan
  const result = await prisma.$queryRaw`
    SELECT
      u."subscriptionPlan",
      AVG(cost_sum.total) as avg_cost,
      COUNT(DISTINCT u.id) as user_count
    FROM "User" u
    LEFT JOIN (
      SELECT "userId", SUM("totalCost") as total
      FROM "LLMUsageLog"
      WHERE "createdAt" >= NOW() - INTERVAL '30 days'
      GROUP BY "userId"
    ) cost_sum ON u.id = cost_sum."userId"
    GROUP BY u."subscriptionPlan"
  `;

  return result;
}

export async function getTotalOperationalCost(dateRange: { from: Date; to: Date }) {
  const total = await prisma.lLMUsageLog.aggregate({
    where: {
      createdAt: {
        gte: dateRange.from,
        lte: dateRange.to
      }
    },
    _sum: {
      totalCost: true
    }
  });

  return Number(total._sum.totalCost || 0);
}
```

---

### Phase 7: Pricing Optimization Analysis

**File**: `docs/05-business/analysis/credit-pricing-analysis.md` (to be created by Agent Planner)

**Key Questions to Answer**:

1. **What's the cost per credit in USD?**
   ```
   Average LLM cost per feature:
   - Character Gen: $0.0011 = 85 credits ‚Üí 1 credit = $0.000013
   - Story Gen: $0.0015 = 100 credits ‚Üí 1 credit = $0.000015
   - Chat Message: $0.00015 = 5 credits ‚Üí 1 credit = $0.00003

   Weighted average: 1 credit ‚âà $0.00002 (LLM cost only)
   ```

2. **What's the target profit margin?**
   ```
   Option A (Conservative): 50% margin
   ‚Üí 1 credit = $0.00004 (includes 2x markup)

   Option B (Aggressive): 30% margin
   ‚Üí 1 credit = $0.000026

   Recommendation: Start with 40% margin ‚Üí 1 credit = $0.000033
   ```

3. **How many credits for PLUS ($5/month)?**
   ```
   PLUS = $5/month
   Target profit: $2/month (40%)
   Available for LLM: $3/month

   At $0.00002/credit ‚Üí 150,000 credits/month
   Daily credits: 5,000 credits/day

   Reality check:
   - 10 character generations = 1,000 credits
   - 50 chat messages = 250 credits
   - User can generate 150 characters/month comfortably
   ```

4. **How many free credits per day is sustainable?**
   ```
   Free users cost: $0/revenue but LLM costs apply

   Conservative: 100 credits/day = $0.002/day = $0.06/month per user
   If 10,000 free users ‚Üí $600/month LLM cost

   Acceptable if conversion rate > 2% (200 paying users = $1,000 revenue)
   ```

---

### Phase 8: API Endpoints

**File**: `backend/src/routes/v1/analytics.ts`

```typescript
// GET /api/v1/analytics/llm-costs/by-feature
router.get('/llm-costs/by-feature', requireAuth, requireAdmin, async (req, res) => {
  const { from, to } = req.query;

  const costs = await llmCostAnalytics.getCostByFeature({
    from: new Date(from as string),
    to: new Date(to as string)
  });

  res.json(costs);
});

// GET /api/v1/analytics/llm-costs/total
router.get('/llm-costs/total', requireAuth, requireAdmin, async (req, res) => {
  const { from, to } = req.query;

  const total = await llmCostAnalytics.getTotalOperationalCost({
    from: new Date(from as string),
    to: new Date(to as string)
  });

  res.json({ total, currency: 'USD' });
});

// GET /api/v1/analytics/llm-costs/by-plan
router.get('/llm-costs/by-plan', requireAuth, requireAdmin, async (req, res) => {
  const costs = await llmCostAnalytics.getAverageCostByPlan();
  res.json(costs);
});
```

---

## Testing Strategy

### Unit Tests
- Test cost calculation logic
- Test pricing lookup
- Test analytics aggregations

### Integration Tests
- Track LLM usage in character generation flow
- Verify costs calculated correctly
- Test analytics queries

### Manual Validation
- Generate 10 characters, verify logs
- Check total cost matches expected
- Review analytics dashboard

---

## Rollout Strategy

### Phase 1: Research & Planning (Agent Planner - 8-10 hours)
1. Web research current LLM pricing (2 hours)
2. Map all features to LLM usage (3 hours)
3. Create cost analysis document (2 hours)
4. Calculate optimal credit pricing (2 hours)
5. Create recommendations report (1 hour)

### Phase 2: Implementation (Agent Coder - 10-12 hours)
1. Database schema (1 hour)
2. Seed pricing data (1 hour)
3. Tracking service (2 hours)
4. Integration with existing services (4 hours)
5. Analytics service (2 hours)
6. API endpoints (1 hour)
7. Testing (1 hour)

### Phase 3: Dashboard (Future - Agent Coder - 8-10 hours)
1. Admin dashboard UI
2. Cost charts and graphs
3. Real-time monitoring

**Total Estimated Time**: 18-22 hours (Planner + Coder)

---

## Success Metrics

**Phase 1 Complete** ‚úÖ:
- [x] All LLM models mapped
- [x] Current pricing documented
- [x] Cost per feature calculated
- [x] Optimal credit pricing defined

**Phase 1.5 Complete** ‚úÖ (Content Filtering):
- [x] Content classification service (Gemini 2.5 Flash-Lite)
- [x] Age validation system (L, TEN, TWELVE, FOURTEEN, SIXTEEN, EIGHTEEN)
- [x] Model router service (feature-based routing)
- [x] OpenRouter integration (Venice AI FREE for chat)
- [x] Chat message filtering
- [x] Character generation filtering
- [x] Character edit filtering

**Phase 2 Complete** ‚úÖ:
- [x] 100% LLM calls tracked
- [x] Costs calculated in real-time
- [x] Dashboard showing costs by feature
- [x] Cost by plan visualization
- [x] Caching metrics display
- [x] Top users by cost
- [x] Daily costs trend
- [x] Full i18n support (12 languages)
- [ ] 30+ days of data collected (IN PROGRESS - started 2026-01-05)

**Phase 3 Pending** üìä:
- [ ] Monthly cost projections accurate within 10%
- [ ] Profitability per plan calculated (awaiting data)
- [ ] Optimization recommendations (awaiting data)

---

## Expected Outcomes

**Cost Optimization**:
- [x] Identify expensive features: Venice AI FREE for chat = $0 cost
- [x] Switch to cheaper models where appropriate (Grok 4-1 instead of Gemini Pro)
- [x] Translation caching implemented (90% cost reduction)
- [ ] Aggressive caching for other features (PENDING - Phase 3)

**Pricing Optimization**:
- [x] Data-driven credit pricing (Venice AI reduces chat cost to $0)
- [ ] Profitable PLUS/PREMIUM plans (PENDING - Phase 3 analysis)
- [ ] Sustainable free tier (PENDING - Phase 3 analysis)

**Content Safety**:
- [x] Age-based content filtering active
- [x] NSFW content blocked for underage users
- [x] Clear error messages for restricted content

**Business Intelligence**:
- [x] Real-time cost tracking
- [x] Dashboard for visualization
- [ ] Monthly cost projections (PENDING - Phase 3)
- [ ] ROI per feature (PENDING - Phase 3)
- [ ] User acquisition cost vs LTV (PENDING - Phase 3)

---

## Risks & Mitigation

### Risk 1: Tracking Overhead
**Mitigation**: Async logging, batched inserts

### Risk 2: Cost Accuracy
**Mitigation**: Regular pricing updates, validation

### Risk 3: Privacy Concerns
**Mitigation**: No prompt content logged, only metadata

---

## Notes for Agent Planner

This feature has evolved during implementation and now includes THREE major components:

**Phase 1 - Cost Tracking (COMPLETED)**:
- ‚úÖ Web research LLM pricing
- ‚úÖ Map features to costs
- ‚úÖ Tracking service implemented
- ‚úÖ All LLM calls tracked in real-time

**Phase 1.5 - Content Safety (COMPLETED - Unplanned Addition)**:
- ‚úÖ Content classification service (Gemini 2.5 Flash-Lite)
- ‚úÖ Age validation system
- ‚úÖ Model router service
- ‚úÖ OpenRouter integration (Venice AI FREE)
- ‚úÖ Chat, character generation, and character edit filtering

**Phase 2 - Analytics & Dashboard (COMPLETED)**:
- ‚úÖ Dashboard UI for cost visualization
- ‚úÖ API endpoints for analytics
- ‚úÖ Full i18n support (12 languages)
- ‚úÖ Real-time data visualization

**Phase 3 - Optimization & Analysis (PENDING - Scheduled for 2026-02-05)**:
- [ ] Collect 30+ days of LLM usage data (STARTED: 2026-01-05)
- [ ] Pricing optimization analysis
- [ ] ROI calculations per plan
- [ ] Cost reduction recommendations

---

## Phase 3: Analytics Analysis Task (For Agent Planner)

**Scheduled Date**: 2026-02-05 (30+ days after data collection started)
**Assigned To**: Agent Planner
**Prerequisites**: 30+ days of LLM usage data collected

### Task Objectives

1. **Cost Analysis by Feature**
   - Identify top 3 most expensive features
   - Calculate cost per request for each feature
   - Analyze cost trends over time

2. **Profitability by Plan**
   - Calculate LLM cost per FREE user
   - Calculate LLM cost per PLUS user
   - Calculate LLM cost per PREMIUM user
   - Determine which plans are profitable

3. **Pricing Optimization**
   - Current credit price vs actual LLM cost
   - Recommended credit pricing for profitability
   - Recommended monthly credits for PLUS/PREMIUM
   - Sustainable daily free tier limit

4. **Cost Reduction Opportunities**
   - Features with highest caching potential
   - Models that can be replaced with cheaper alternatives
   - Target: 20% cost reduction without quality loss

5. **Projections & Recommendations**
   - Monthly cost projections (conservative, moderate, aggressive growth)
   - ROI per user acquisition channel
   - Break-even analysis by plan
   - Actionable recommendations for pricing

### Deliverables

1. **Analysis Report** (`docs/05-business/analysis/llm-cost-analysis-2026-02.md`)
   - Executive summary
   - Cost breakdown by feature
   - Profitability analysis by plan
   - Pricing recommendations
   - Cost reduction opportunities

2. **Updated Pricing Configuration**
   - Recommended credit prices
   - Recommended plan quotas
   - Recommended free tier limits

3. **Implementation Tasks** (create new feature specs if needed)
   - Pricing changes to implement
   - Additional caching to add
   - Model swaps to consider

### Data Sources

- **Dashboard**: `/admin/analytics` (requires ADMIN role)
- **API**: `/api/v1/admin/analytics/llm/*`
- **Database**: `LLMUsageLog`, `LLMPricing` tables

---

**End of Specification**

üìä Cost tracking: COMPLETE | üõ°Ô∏è Content filtering: COMPLETE | üìà Analytics dashboard: COMPLETE | üìã Phase 3 analysis: SCHEDULED FOR 2026-02-05
