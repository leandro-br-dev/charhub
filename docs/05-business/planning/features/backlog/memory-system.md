# Sistema de Mem√≥ria e Compacta√ß√£o de Contexto

**Criado em**: 2025-11-20
**Status**: ‚úÖ Implementado e Funcional
**Vers√£o**: 1.0

---

## üìã √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Problema Resolvido](#problema-resolvido)
3. [Arquitetura](#arquitetura)
4. [Configura√ß√£o](#configura√ß√£o)
5. [Fluxo de Funcionamento](#fluxo-de-funcionamento)
6. [Estrutura de Dados](#estrutura-de-dados)
7. [API e M√©todos](#api-e-m√©todos)
8. [Testes e Valida√ß√£o](#testes-e-valida√ß√£o)
9. [Monitoramento](#monitoramento)
10. [FAQ](#faq)

---

## Vis√£o Geral

O Sistema de Mem√≥ria do CharHub resolve o problema de **contexto limitado** em conversas longas com LLMs. Ele automaticamente:

1. Monitora o tamanho do contexto da conversa (em tokens)
2. Quando atinge o limite, **compacta** o hist√≥rico antigo em um resumo
3. Mant√©m apenas as **√∫ltimas 10 mensagens** completas
4. Usa **hist√≥rico compactado + mensagens recentes** como contexto para novas respostas

**Resultado**: Conversas podem ter **tamanho ilimitado** sem perder coer√™ncia.

---

## Problema Resolvido

### Antes (Sem Sistema de Mem√≥ria)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONTEXTO DO LLM                    ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Ä¢ Todas as mensagens (1-100)      ‚îÇ ‚Üê Limite atingido!
‚îÇ  ‚Ä¢ Total: 10,000 tokens            ‚îÇ ‚Üê Muito caro
‚îÇ                                     ‚îÇ ‚Üê Perde mensagens antigas
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Problemas**:
- ‚ùå Limite de contexto atingido (~8k-32k tokens)
- ‚ùå Custo crescente com cada mensagem
- ‚ùå Performance degradada (lat√™ncia)
- ‚ùå Perda de informa√ß√µes antigas quando limite √© atingido

### Depois (Com Sistema de Mem√≥ria)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONTEXTO DO LLM                    ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ HIST√ìRICO COMPACTADO (30%)    ‚îÇ ‚îÇ ‚Üê 2,400 tokens
‚îÇ  ‚îÇ "Alice e Bob se encontraram..." ‚îÇ
‚îÇ  ‚îÇ "Eventos principais: ..."      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ √öLTIMAS 10 MENSAGENS (70%)    ‚îÇ ‚îÇ ‚Üê 1,500 tokens
‚îÇ  ‚îÇ Msg 91: "Ol√°..."              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Msg 92: "Como vai?"           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ...                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Msg 100: "At√© logo!"          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Total: ~3,900 tokens (dentro limite)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Benef√≠cios**:
- ‚úÖ Conversas ilimitadas
- ‚úÖ Custo reduzido (~60% economia)
- ‚úÖ Performance mantida
- ‚úÖ Coer√™ncia preservada (LLM "lembra" do hist√≥rico)

---

## Arquitetura

### Componentes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FLUXO DE MENSAGENS                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. ChatHandler (websocket)                                  ‚îÇ
‚îÇ     ‚Ä¢ Recebe mensagem do usu√°rio                            ‚îÇ
‚îÇ     ‚Ä¢ Salva no banco de dados                               ‚îÇ
‚îÇ     ‚Ä¢ Verifica: shouldCompressMemory()                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº (se precisar compactar)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. BullMQ Queue (ass√≠ncrono)                               ‚îÇ
‚îÇ     ‚Ä¢ Enfileira job de compacta√ß√£o                          ‚îÇ
‚îÇ     ‚Ä¢ N√£o bloqueia resposta do chat                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. MemoryCompressionWorker                                 ‚îÇ
‚îÇ     ‚Ä¢ Processa job em background                            ‚îÇ
‚îÇ     ‚Ä¢ Chama: memoryService.compressConversationMemory()     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. MemoryService.generateMemory()                          ‚îÇ
‚îÇ     ‚Ä¢ Busca mensagens antigas (exceto √∫ltimas 10)           ‚îÇ
‚îÇ     ‚Ä¢ Monta contexto + resumo anterior                      ‚îÇ
‚îÇ     ‚Ä¢ Chama LLM (Gemini Flash) para resumir                 ‚îÇ
‚îÇ     ‚Ä¢ Salva resumo no banco (ConversationMemory)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. ResponseGenerationAgent                                 ‚îÇ
‚îÇ     ‚Ä¢ Chama: memoryService.buildContextWithMemory()         ‚îÇ
‚îÇ     ‚Ä¢ Retorna: Resumo + √öltimas 10 mensagens                ‚îÇ
‚îÇ     ‚Ä¢ Usa como contexto para gerar resposta                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Schema do Banco de Dados

```prisma
model Conversation {
  id                   String              @id @default(uuid())
  // ... outros campos
  memoryLastUpdatedAt  DateTime?           // Timestamp da √∫ltima compacta√ß√£o
  memories             ConversationMemory[] // Rela√ß√£o com resumos
}

model ConversationMemory {
  id             String   @id @default(uuid())
  conversationId String
  conversation   Conversation @relation(fields: [conversationId], references: [id])

  // Conte√∫do do resumo
  summary        String   @db.Text          // Resumo em prosa
  keyEvents      Json                       // Array de eventos importantes
  messageCount   Int                        // Quantas mensagens foram resumidas

  // Metadata
  startMessageId String?                    // Primeira mensagem resumida
  endMessageId   String?                    // √öltima mensagem resumida
  createdAt      DateTime @default(now())
  updatedAt      DateTime @updatedAt

  @@index([conversationId])
  @@index([createdAt])
}
```

---

## Configura√ß√£o

### Vari√°veis de Ambiente

Adicione ao `.env` (backend):

```bash
# Limite m√°ximo de tokens no contexto (padr√£o: 8000)
MAX_CONTEXT_TOKENS=8000
```

### Constantes do Sistema

Em `backend/src/services/memoryService.ts`:

```typescript
const MAX_CONTEXT_TOKENS = 8000;              // Janela total de contexto
const MAX_COMPRESSED_TOKENS = 2400;           // 30% para hist√≥rico compactado
const RECENT_MESSAGES_COUNT = 10;             // √öltimas N mensagens completas
const AVG_TOKENS_PER_MESSAGE = 150;           // Estimativa de tokens/msg
```

**C√°lculo**:
- `MAX_COMPRESSED_TOKENS = MAX_CONTEXT_TOKENS * 0.30` (30%)
- `Tokens dispon√≠veis para mensagens = MAX_CONTEXT_TOKENS * 0.70` (70%)

### Ajustando Limites

Para modelos com contexto maior (ex: GPT-4 Turbo = 128k tokens):

```bash
MAX_CONTEXT_TOKENS=32000  # Usa apenas 32k dos 128k dispon√≠veis
```

**Recomenda√ß√µes**:
- **Gemini 1.5 Flash**: 8,000 tokens (padr√£o)
- **GPT-3.5 Turbo**: 4,000 tokens
- **GPT-4 Turbo**: 32,000 tokens (usar menos para economizar)
- **Claude 3**: 16,000 tokens

---

## Fluxo de Funcionamento

### 1. Envio de Mensagem (User ‚Üí Backend)

```javascript
// Frontend envia mensagem via WebSocket
socket.emit('send_message', {
  conversationId: 'abc-123',
  content: 'Ol√°!',
});
```

### 2. Verifica√ß√£o de Limite (Backend)

```typescript
// chatHandler.ts (ap√≥s salvar mensagem)
const { memoryService } = await import('../services/memoryService');

// Verifica se precisa compactar (n√£o bloqueia resposta)
memoryService.shouldCompressMemory(conversationId).then(async (shouldCompress) => {
  if (shouldCompress) {
    // Enfileira compacta√ß√£o (ass√≠ncrono)
    const memoryQueue = getQueue(QueueName.MEMORY_COMPRESSION);
    await memoryQueue.add('compress-memory', { conversationId });

    // Notifica frontend
    io.to(room).emit('memory_compression_started', { conversationId });
  }
});
```

### 3. Compacta√ß√£o (Worker Ass√≠ncrono)

```typescript
// memoryCompressionWorker.ts
export async function processMemoryCompression(job) {
  const { conversationId } = job.data;

  // 1. Buscar mensagens antigas (exceto √∫ltimas 10)
  const messages = await fetchMessagesToCompress(conversationId);

  // 2. Montar prompt para LLM
  const prompt = `
    [Resumo anterior se existir]

    Novas mensagens:
    ${messages.map(m => `${m.sender}: ${m.content}`).join('\n')}

    Resuma em m√°ximo 2400 tokens (JSON):
    { summary: "...", keyEvents: [...] }
  `;

  // 3. Chamar LLM (Gemini Flash)
  const result = await callLLM({
    provider: 'gemini',
    model: 'gemini-2.5-flash',
    prompt,
    temperature: 0.3,
    maxTokens: 2000
  });

  // 4. Salvar resumo no banco
  await prisma.conversationMemory.create({
    data: {
      conversationId,
      summary: result.summary,
      keyEvents: result.keyEvents,
      messageCount: messages.length,
      startMessageId: messages[0].id,
      endMessageId: messages[messages.length - 1].id
    }
  });

  // 5. Atualizar timestamp na conversa
  await prisma.conversation.update({
    where: { id: conversationId },
    data: { memoryLastUpdatedAt: new Date() }
  });
}
```

### 4. Gera√ß√£o de Resposta (Usa Contexto Compactado)

```typescript
// responseGenerationAgent.ts
export async function execute(conversation, user, lastMessage) {
  // Buscar contexto: Resumo + √öltimas 10 mensagens
  const historyContext = await memoryService.buildContextWithMemory(conversation.id);

  // historyContext cont√©m:
  // [= CONVERSATION HISTORY (SUMMARIZED) =]
  // Summary 1 (30 messages):
  // Alice and Bob met at the park. They discussed...
  //
  // Key Events:
  // - Alice confessed her feelings (high)
  // - Bob agreed to go on a date (medium)
  //
  // [= END OF SUMMARIZED HISTORY =]
  //
  // [= RECENT MESSAGES (FULL CONTEXT) =]
  // Alice: So, when should we meet?
  // Bob: How about tomorrow at 3pm?
  // ...

  // Gerar resposta usando esse contexto
  const response = await callLLM({
    systemPrompt: `You are ${character.name}. ${character.description}`,
    userPrompt: `${historyContext}\n\nUser: ${lastMessage.content}\n\nYou:`,
    // ...
  });

  return response.content;
}
```

---

## Estrutura de Dados

### Exemplo de Resumo Salvo

```json
{
  "id": "mem-456",
  "conversationId": "conv-123",
  "summary": "Alice and Bob met at a coffee shop. They discussed their favorite books and discovered they both love sci-fi. Alice mentioned she's working on a novel.",
  "keyEvents": [
    {
      "timestamp": "2025-11-20T10:15:00Z",
      "description": "Alice and Bob met for the first time",
      "participants": ["Alice", "Bob"],
      "importance": "high"
    },
    {
      "timestamp": "2025-11-20T10:30:00Z",
      "description": "Bob recommended 'Dune' to Alice",
      "participants": ["Bob", "Alice"],
      "importance": "medium"
    }
  ],
  "messageCount": 35,
  "startMessageId": "msg-1",
  "endMessageId": "msg-35",
  "createdAt": "2025-11-20T11:00:00Z"
}
```

---

## API e M√©todos

### MemoryService

#### `shouldCompressMemory(conversationId: string): Promise<boolean>`

Verifica se a conversa atingiu o limite de tokens.

**Retorna**: `true` se precisa compactar, `false` caso contr√°rio.

**L√≥gica**:
```typescript
const tokenStats = await calculateContextTokens(conversationId);
return tokenStats.totalTokens >= MAX_CONTEXT_TOKENS &&
       tokenStats.recentMessageCount > RECENT_MESSAGES_COUNT;
```

---

#### `compressConversationMemory(conversationId: string): Promise<boolean>`

Executa compacta√ß√£o completa: gera resumo e salva no banco.

**Retorna**: `true` se sucesso, `false` se falhou.

---

#### `buildContextWithMemory(conversationId: string, recentMessageLimit?: number): Promise<string>`

Constr√≥i contexto completo para LLM.

**Retorna**: String formatada com resumo + mensagens recentes.

**Formato**:
```
[= CONVERSATION HISTORY (SUMMARIZED) =]
Summary 1 (30 messages): ...
Key Events: ...

[= END OF SUMMARIZED HISTORY =]

[= RECENT MESSAGES (FULL CONTEXT) =]
Alice: ...
Bob: ...
```

---

#### `calculateContextTokens(conversationId: string): Promise<TokenStats>`

Calcula estat√≠sticas de tokens.

**Retorna**:
```typescript
{
  compressedTokens: number;      // Tokens nos resumos
  recentMessagesTokens: number;  // Tokens nas mensagens recentes
  totalTokens: number;           // Total
  recentMessageCount: number;    // Contagem de mensagens recentes
}
```

---

## Testes e Valida√ß√£o

### Teste Manual

1. **Criar conversa longa** (>60 mensagens):
```bash
# Via frontend, enviar m√∫ltiplas mensagens
for i in {1..70}; do
  echo "Mensagem $i: Lorem ipsum dolor sit amet..."
done
```

2. **Verificar logs** do backend:
```bash
docker compose logs -f backend | grep "memory"

# Deve aparecer:
# "Context limit reached, queuing memory compression"
# "Starting memory compression job"
# "Memory compression completed successfully"
```

3. **Verificar banco de dados**:
```sql
SELECT * FROM "ConversationMemory" WHERE "conversationId" = 'conv-123';

-- Deve ter pelo menos 1 registro
```

4. **Testar contexto**:
```bash
# Enviar nova mensagem e verificar resposta
# LLM deve "lembrar" de eventos do resumo
```

---

### Testes Automatizados (TODO)

```typescript
// backend/tests/services/memoryService.test.ts
describe('MemoryService', () => {
  it('should compress memory when context limit is reached', async () => {
    const conversationId = await createTestConversation();
    await createTestMessages(conversationId, 70); // Acima do limite

    const shouldCompress = await memoryService.shouldCompressMemory(conversationId);
    expect(shouldCompress).toBe(true);

    await memoryService.compressConversationMemory(conversationId);

    const memories = await memoryService.getConversationMemories(conversationId);
    expect(memories.length).toBeGreaterThan(0);
  });

  it('should build context with compressed history + recent messages', async () => {
    // ... test implementation
  });
});
```

---

## Monitoramento

### M√©tricas Importantes

1. **Taxa de Compacta√ß√£o**:
   - Quantas conversas atingem o limite?
   - Frequ√™ncia de compacta√ß√µes por conversa

2. **Performance**:
   - Tempo m√©dio de compacta√ß√£o
   - Lat√™ncia de gera√ß√£o de resumo (LLM)

3. **Custo**:
   - Tokens usados para resumos (vs. contexto completo)
   - Economia estimada

4. **Qualidade**:
   - Resumos preservam informa√ß√µes importantes?
   - LLM mant√©m coer√™ncia com hist√≥rico compactado?

### Logs para Monitorar

```bash
# Ver compacta√ß√µes
docker compose logs -f backend | grep "memory compression"

# Ver estat√≠sticas de tokens
docker compose logs -f backend | grep "Context token stats"

# Ver erros
docker compose logs -f backend | grep "Error.*memory"
```

### Dashboard (TODO)

Criar dashboard no frontend com:
- Indicador de mem√≥ria ativa (MemoryIndicator j√° implementado)
- Estat√≠sticas de tokens usados
- Hist√≥rico de compacta√ß√µes
- Op√ß√£o para for√ßar compacta√ß√£o manual

---

## FAQ

### 1. O que acontece se o resumo ficar muito grande (>30% dos tokens)?

O prompt do LLM limita explicitamente: "use at most 2400 tokens". Se ultrapassar, o LLM ser√° instru√≠do novamente a resumir.

---

### 2. Posso desabilitar o sistema de mem√≥ria?

Sim. No `responseGenerationAgent.ts`, comente a linha:

```typescript
// const historyContext = await memoryService.buildContextWithMemory(conversation.id);

// E use o m√©todo antigo:
const formattedHistory = formatConversationHistoryForLLM(
  conversation.messages,
  conversation.participants
);
const historyContext = formattedHistory
  .map((entry) => `${entry.sender_name}: ${entry.content}`)
  .join('\n');
```

---

### 3. Como ajustar para modelos com contexto maior (ex: Claude 100k)?

Aumentar `MAX_CONTEXT_TOKENS`:

```bash
MAX_CONTEXT_TOKENS=32000  # Usar 32k dos 100k dispon√≠veis
```

Nota: Usar menos que o m√°ximo reduz custos.

---

### 4. O sistema funciona com m√∫ltiplos personagens?

Sim! O resumo inclui nomes de todos os participantes nos eventos.

---

### 5. Quanto custa cada compacta√ß√£o?

Estimativa com Gemini 2.5 Flash:
- Input: ~3000 tokens (mensagens antigas)
- Output: ~500 tokens (resumo)
- **Custo**: ~$0.003 por compacta√ß√£o

Economia por conversa longa: ~60% vs. contexto completo.

---

### 6. O que acontece se a compacta√ß√£o falhar?

Fallback: usa √∫ltimas 10 mensagens sem resumo. N√£o quebra o chat.

---

### 7. Posso ver o hist√≥rico compactado?

Sim! Via Prisma Studio ou endpoint:

```bash
GET /api/v1/conversations/:id/memories
```

(Endpoint ainda n√£o implementado, adicionar se necess√°rio)

---

## Pr√≥ximas Melhorias

### Fase 1 (Curto Prazo)
- [ ] Adicionar endpoint `GET /conversations/:id/memories` para visualizar resumos
- [ ] Criar UI no frontend para mostrar timeline de compacta√ß√µes
- [ ] Adicionar testes automatizados

### Fase 2 (M√©dio Prazo)
- [ ] Usar modelo de embeddings para busca sem√¢ntica em hist√≥rico
- [ ] Permitir "pinnar" mensagens importantes (nunca compactar)
- [ ] Dashboard com estat√≠sticas de tokens e custos

### Fase 3 (Longo Prazo)
- [ ] Compacta√ß√£o incremental (evitar resumir o mesmo conte√∫do)
- [ ] Suporte a anexos (imagens, √°udio) no hist√≥rico compactado
- [ ] Sistema de "refresh" de mem√≥ria (re-resumir hist√≥rico antigo)

---

## Conclus√£o

O Sistema de Mem√≥ria permite conversas **ilimitadas** no CharHub, mantendo:
- ‚úÖ Coer√™ncia narrativa
- ‚úÖ Performance otimizada
- ‚úÖ Custos reduzidos (~60% economia)
- ‚úÖ Experi√™ncia transparente para o usu√°rio

**Status**: ‚úÖ Implementado e Pronto para Produ√ß√£o

---

**√öltima atualiza√ß√£o**: 2025-11-20
**Autor**: Claude (AI Assistant) + Leandro (Product Owner)
**Vers√£o**: 1.0
