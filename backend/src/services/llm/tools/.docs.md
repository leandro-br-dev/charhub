# LLM Tool-Calling System Documentation

**Purpose**: Enable LLMs to access real-time information and execute actions beyond their training data.

**Related Files**:
- Service: `backend/src/services/llm/index.ts`
- Tools: `backend/src/services/llm/tools/`
- Routes: `backend/src/routes/v1/llm-test.ts`
- Used by: Character autocomplete agent, chat agents

## Overview

The tool-calling system allows LLMs to extend their capabilities by calling external tools and APIs.

### Current Capabilities

- **Web Search**: Real-time web search via DuckDuckGo API
- **Multi-Provider**: Support for OpenAI and Gemini (Grok not supported)
- **Auto-Execution**: Automatic tool execution with result aggregation
- **Rate Limiting**: Token bucket algorithm for abuse prevention
- **Caching**: In-memory cache to reduce costs and latency

### Supported Providers

| Provider | Tool Support | Implementation |
|----------|--------------|----------------|
| OpenAI | ✅ Yes | Function calling |
| Gemini | ✅ Yes | Function declarations |
| Grok | ❌ No | Not supported |

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    LLM Service (index.ts)                    │
│  - Provider routing                                          │
│  - Tool preparation                                         │
│  - Optional auto-execution                                   │
└───────────────┬─────────────────────────────────────────────┘
                │
                ├─────────────────┬─────────────────┬──────────
                ▼                 ▼                 ▼
        ┌───────────────┐ ┌───────────────┐ ┌───────────────┐
        │ OpenAI Adapter│ │ Gemini Adapter│ │  Grok Adapter │
        │ (function     │ │ (function     │ │ (no support)  │
        │  calling)     │ │  declarations)│ │               │
        └───────────────┘ └───────────────┘ └───────────────┘
                │                 │
                └────────┬────────┘
                         ▼
                ┌─────────────────┐
                │  Tool Registry  │
                │  (tools/index)  │
                └────────┬────────┘
                         │
                ┌────────┴────────┐
                ▼                 ▼
        ┌──────────────┐  ┌──────────────┐
        │  Web Search  │  │ Future Tools │
        │   Tool       │  │  (planned)   │
        └──────────────┘  └──────────────┘
```

## API/Usage

### Tool Definition Interface

```typescript
export interface ToolDefinition {
  name: string;                  // Unique tool name
  description: string;           // What the tool does
  parameters: {                  // JSON Schema of parameters
    type: string;
    properties: Record<string, any>;
    required: string[];
  };
  execute: (args: any) => Promise<string>; // Execution function
}
```

### Using Tools in LLM Calls

```typescript
import { callLLM } from '../services/llm';

const response = await callLLM({
  provider: 'gemini',
  model: 'gemini-2.0-flash-exp',
  systemPrompt: 'You are a helpful assistant.',
  userPrompt: 'What is the current weather in Tokyo?',

  // Option 1: Specify tools manually
  tools: ['web_search'],
  toolChoice: 'auto', // 'auto' | 'none' | 'required'

  // Option 2: Shortcut for web search
  allowBrowsing: true,

  // Auto-execute tools
  autoExecuteTools: true,
});

// Response contains:
// - response.content: Generated text
// - response.toolCalls: Tools called by LLM
// - response.toolResults: Execution results (if autoExecuteTools=true)
```

### Execution Flow

```
1. User calls callLLM() with tools enabled
   ↓
2. LLM Service prepares tool definitions
   ↓
3. Provider adapter sends to LLM API
   ↓
4. LLM decides if it needs to use tools
   ↓
5. If YES: Returns tool calls (name + arguments)
   ↓
6. If autoExecuteTools=true:
   - Tool Registry executes each tool
   - Results aggregated and returned
   ↓
7. Final response with content + toolCalls + toolResults
```

## Web Search Tool

### Characteristics

- **API**: DuckDuckGo Instant Answer API (free, no API key required)
- **Cache**: In-memory with 1-hour TTL
- **Rate Limiting**: Token bucket (10 tokens, 1 token/second)
- **Results**: Up to 5 results with title, URL, and snippet

### Usage Example

```typescript
// In agents
const llmResponse = await callLLM({
  provider: 'gemini',
  model: 'gemini-2.5-flash-lite',
  systemPrompt: buildSystemPrompt(mode), // Includes web search instructions
  userPrompt: buildUserPrompt(input),
  allowBrowsing: mode === 'web',       // Enable web search in 'web' mode
  autoExecuteTools: mode === 'web',    // Auto-execute searches
});
```

### Implementation Details

```typescript
export async function webSearch(query: string): Promise<WebSearchResponse> {
  // 1. Check cache
  const cached = searchCache.get(query.toLowerCase().trim());
  if (cached && cached.expires > Date.now()) {
    return cached.data;
  }

  // 2. Rate limiting
  if (!consumeToken()) {
    throw new Error('Rate limit exceeded');
  }

  // 3. Fetch from DuckDuckGo API
  const url = `https://api.duckduckgo.com/?q=${encodeURIComponent(query)}&format=json`;
  const response = await fetch(url);
  const data = await response.json();

  // 4. Parse results
  const results = parseResults(data);

  // 5. Cache and return
  cacheResult(query, results);
  return results;
}
```

## Adding New Tools

### Step 1: Create Tool File

```typescript
// backend/src/services/llm/tools/myNewTool.ts

export interface MyToolParams {
  param1: string;
  param2?: number;
}

export async function executeMyTool(params: MyToolParams): Promise<string> {
  // Implement tool logic
  const result = await doSomething(params.param1);

  return JSON.stringify({
    result,
    metadata: { /* ... */ }
  });
}

export const myNewToolDefinition: ToolDefinition = {
  name: 'my_new_tool',
  description: 'Description of what this tool does',
  parameters: {
    type: 'object',
    properties: {
      param1: {
        type: 'string',
        description: 'Description of param1',
      },
      param2: {
        type: 'number',
        description: 'Optional param2',
      },
    },
    required: ['param1'],
  },
  execute: async (args: any) => {
    return executeMyTool(args as MyToolParams);
  },
};
```

### Step 2: Register in Tool Registry

```typescript
// backend/src/services/llm/tools/index.ts

import { myNewToolDefinition } from './myNewTool';

export const availableTools: Record<string, ToolDefinition> = {
  web_search: webSearchTool,
  my_new_tool: myNewToolDefinition, // ← Add here
};
```

### Step 3: Use in Agents or Endpoints

```typescript
const response = await callLLM({
  provider: 'gemini',
  model: 'gemini-2.0-flash-exp',
  systemPrompt: 'You can use my_new_tool to do X.',
  userPrompt: 'Please do X',
  tools: ['my_new_tool'],  // ← Specify tool
  autoExecuteTools: true,
});
```

## Dependencies

- **DuckDuckGo API**: Free, no API key required
- **LLM Service**: Core LLM calling functionality
- **In-Memory Cache**: Map-based caching

## Important Notes

### Best Practices

**DO**:
- Always instruct LLM when and how to use tools in system prompt
- Implement rate limiting for tools with external calls
- Implement caching to reduce costs and latency
- Return errors safely from tool execution
- Log tool executions for debugging

**DON'T**:
- Skip rate limiting for external API calls
- Forget to normalize queries before caching
- Return raw errors from tools (wrap safely)
- Use tools without clear system prompt instructions

### System Prompt Guidelines

```typescript
const systemPrompt = [
  'You are a helpful assistant.',
  'You have access to web_search tool.',
  'Use web search when:',
  '- You need current or recent information',
  '- Facts that may have changed since your training',
  '- Real-time data like weather, news, stock prices',
  'Do NOT use web search for:',
  '- General knowledge questions',
  '- Historical facts',
  '- Theoretical concepts',
].join('\n');
```

### Rate Limiting

Token bucket algorithm implementation:

```typescript
let tokens = MAX_TOKENS;
let lastRefill = Date.now();

function consumeToken(): boolean {
  refillTokens();
  if (tokens > 0) {
    tokens--;
    return true;
  }
  return false;
}

function refillTokens() {
  const now = Date.now();
  const elapsed = (now - lastRefill) / 1000;
  const newTokens = Math.floor(elapsed * REFILL_RATE);

  if (newTokens > 0) {
    tokens = Math.min(MAX_TOKENS, tokens + newTokens);
    lastRefill = now;
  }
}
```

### Caching Strategy

```typescript
const cache = new Map<string, { data: any; expires: number }>();

function getCached<T>(key: string): T | null {
  const entry = cache.get(key);
  if (entry && entry.expires > Date.now()) {
    return entry.data as T;
  }
  cache.delete(key);
  return null;
}

function setCache<T>(key: string, data: T, ttl: number) {
  cache.set(key, {
    data,
    expires: Date.now() + ttl,
  });

  // Eviction policy
  if (cache.size > MAX_CACHE_SIZE) {
    const firstKey = cache.keys().next().value;
    cache.delete(firstKey);
  }
}
```

## Testing

### Test Tool Calling

```http
POST /api/v1/llm-test/tool-calling
Authorization: Bearer {token}
Content-Type: application/json

{
  "query": "What is the current weather in Tokyo?",
  "provider": "gemini",
  "autoExecute": true
}
```

### Test Character Autocomplete with Web Search

```http
POST /api/v1/llm-test/character-autocomplete
Authorization: Bearer {token}
Content-Type: application/json

{
  "firstName": "Sherlock",
  "mode": "web"
}
```

## Performance and Costs

### Web Search Tool

| Metric | Value |
|---------|-------|
| **Latency (Cache Hit)** | < 1ms |
| **Latency (Cache Miss)** | 200-500ms |
| **Rate Limit** | 10 req/s |
| **Cache TTL** | 1 hour |
| **API Cost** | $0 (DuckDuckGo is free) |

### LLM Tool Calling Overhead

| Provider | Token Overhead | Extra Latency |
|----------|----------------|---------------|
| **OpenAI** | ~50-100 tokens (tool schemas) | ~100ms |
| **Gemini** | ~30-80 tokens (function declarations) | ~50ms |
| **Grok** | N/A (no support) | N/A |

## Troubleshooting

**Tool calls not being returned**
- Check if provider supports tool calling (Grok doesn't)
- Verify system prompt instructs LLM to use tools
- Check `toolChoice` is not set to `'none'`

**Rate limit exceeded**
- Increase `MAX_TOKENS` in `webSearch.ts`
- Increase `REFILL_RATE` for more tokens/second
- Implement queue system for requests

**Excessive cache misses**
- Normalize queries before caching
- Increase cache TTL
- Increase max cache size

## See Also

- **LLM Service**: `backend/src/services/llm/.docs.md`
- **LLM Routes**: `backend/src/routes/v1/llm.ts`
- **Character Autocomplete Agent**: `backend/src/agents/characterAutocompleteAgent.ts`
- **Test Endpoints**: `backend/src/routes/v1/llm-test.ts`
