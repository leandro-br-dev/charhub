# LLM Service Documentation

**Purpose**: Multi-provider LLM management supporting Google Gemini, OpenAI, and XAI Grok.

**Related Files**:
- Service: `backend/src/services/llm/index.ts`
- Providers: `backend/src/services/llm/gemini.ts`, `openai.ts`, `grok.ts`
- Config: `backend/src/data/llm-models.json`
- Routes: `backend/src/routes/v1/llm.ts`
- Usage Tracker: `backend/src/services/llm/llmUsageTracker.ts`
- Used by: Character autocomplete, chat, story generation, agents

## Overview

The LLM service provides a unified interface for interacting with multiple LLM providers, abstracting away provider-specific differences.

### Supported Providers

| Provider | Status | Models | Special Features |
|----------|--------|--------|------------------|
| Google Gemini | ✅ Full | Flash, Flash Lite, Pro | Tool calling, fast models |
| OpenAI | ✅ Full | GPT-5, GPT-4.1 | Tool calling, reasoning models |
| XAI Grok | ✅ Full | Grok-4, Grok-4-fast | Real-time web search in model |

### Model Classification (October 2025)

**Fast Models** (Optimized for speed and cost):
- `gemini-2.5-flash-lite` - Fastest Gemini model
- `gpt-5-nano` - Efficient GPT-5 variant
- `gpt-4.1-nano` - Fast and affordable (80.1% MMLU)
- `grok-4-fast-non-reasoning` - Grok 4 without reasoning mode

**Medium Performance** (Balanced quality/speed):
- `gemini-2.5-flash` - Best value Gemini model
- `gemini-2.0-flash` - New generation features
- `gpt-5-mini` - Balanced GPT-5
- `grok-code-fast-1` - Code-specialized

**High Performance** (Maximum quality):
- `gemini-2.5-pro` - Most intelligent with adaptive thinking
- `gpt-5` - Best for code and agentic tasks (74.9% SWE-bench)
- `gpt-realtime` - Advanced speech-to-speech
- `grok-4` - Most intelligent with real-time search
- `grok-4-fast-reasoning` - Frontier performance with efficiency

## Architecture

### Service Structure

```
LLM Service (index.ts)
    │
    ├─ Model Router
    │   └─ Routes to appropriate provider
    │
    ├─ Provider Adapters
    │   ├─ Gemini Adapter
    │   ├─ OpenAI Adapter
    │   └─ Grok Adapter
    │
    ├─ Tool Calling System
    │   └─ See: tools/.docs.md
    │
    └─ Usage Tracking
        └─ llmUsageTracker.ts
```

### Configuration

Models are configured in `backend/src/data/llm-models.json`:

```json
{
  "gemini": {
    "name": "Google Gemini",
    "models": {
      "gemini-2.5-flash-lite": { "category": "fast" },
      "gemini-2.5-flash": { "category": "medium" },
      "gemini-2.5-pro": { "category": "high" }
    }
  },
  "openai": {
    "name": "OpenAI",
    "models": {
      "gpt-5-nano": { "category": "fast" },
      "gpt-5-mini": { "category": "medium" },
      "gpt-5": { "category": "high", "noTemperature": true }
    }
  }
}
```

## Configuration

### API Keys

Add to `.env`:

```bash
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
GROK_API_KEY=your_grok_api_key_here
```

**Where to get keys**:
- **Google Gemini**: https://ai.google.dev/
- **OpenAI**: https://platform.openai.com/api-keys
- **XAI Grok**: https://console.x.ai/

**Key formats**:
- **Gemini**: Starts with `AIza...`
- **OpenAI**: Starts with `sk-proj-...` or `sk-...`
- **Grok**: Starts with `xai-...` (⚠️ Don't confuse with Anthropic keys starting with `sk-ant-`)

## API/Usage

### Basic LLM Call

```typescript
import { callLLM } from '../services/llm';

const response = await callLLM({
  provider: 'gemini',
  model: 'gemini-2.5-flash',
  systemPrompt: 'You are a helpful assistant.',
  userPrompt: 'What is the capital of France?',
  temperature: 0.7,
  maxTokens: 1000
});

// Response structure:
// {
//   content: string;
//   model: string;
//   provider: string;
//   usage?: {
//     promptTokens: number;
//     completionTokens: number;
//     totalTokens: number;
//   };
// }
```

### With Tool Calling

```typescript
const response = await callLLM({
  provider: 'gemini',
  model: 'gemini-2.0-flash-exp',
  systemPrompt: 'You can use web_search for current information.',
  userPrompt: 'What is the current weather in Tokyo?',
  tools: ['web_search'],
  toolChoice: 'auto', // 'auto' | 'none' | 'required'
  allowBrowsing: true, // Shortcut for web_search
  autoExecuteTools: true,
});

// Response includes:
// - response.toolCalls: Tools called by LLM
// - response.toolResults: Execution results
```

### Provider Selection

```typescript
// Automatic provider selection based on model
const response = await callLLM({
  model: 'gemini-2.5-flash', // Provider inferred from model name
  userPrompt: 'Hello'
});

// Explicit provider selection
const response = await callLLM({
  provider: 'openai',
  model: 'gpt-5-mini',
  userPrompt: 'Hello'
});
```

## Available Endpoints

### List All Models

```http
GET /api/v1/llm/models
```

**Response**:
```json
{
  "success": true,
  "data": {
    "gemini": {
      "name": "Google Gemini",
      "models": { ... }
    },
    "openai": {
      "name": "OpenAI",
      "models": { ... }
    },
    "grok": {
      "name": "XAI Grok",
      "models": { ... }
    }
  }
}
```

### List Provider Models

```http
GET /api/v1/llm/models/:provider
```

**Example**:
```bash
curl http://localhost/api/v1/llm/models/gemini
```

### Generate Response

```http
POST /api/v1/llm/chat
Content-Type: application/json
```

**Body**:
```json
{
  "provider": "gemini",
  "model": "gemini-2.5-flash",
  "systemPrompt": "You are a helpful assistant.",
  "userPrompt": "What is the capital of France?",
  "temperature": 0.7,
  "maxTokens": 1000
}
```

**Response**:
```json
{
  "success": true,
  "data": {
    "provider": "gemini",
    "model": "gemini-2.5-flash",
    "content": "The capital of France is Paris.",
    "usage": {
      "promptTokens": 15,
      "completionTokens": 8,
      "totalTokens": 23
    }
  }
}
```

## Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `provider` | string | ✅ | `gemini`, `openai`, or `grok` |
| `model` | string | ✅ | Model name (see `/api/v1/llm/models`) |
| `userPrompt` | string | ✅ | User message |
| `systemPrompt` | string | ❌ | System instructions |
| `temperature` | number | ❌ | 0.0 to 1.0 (default: 0.7) - **Not supported in GPT-5** |
| `maxTokens` | number | ❌ | Max response tokens - **Not supported in GPT-5** |

### Model Limitations

**GPT-5 and GPT-Realtime**:
- Don't accept `temperature`, `top_p`, or `maxTokens` parameters
- Use default values only (temperature=1)
- These are reasoning models with fixed parameters

**Other Models** (Gemini, Grok, GPT-4.1):
- Support all parameters normally

## Dependencies

- **@google/generative-ai**: Gemini SDK
- **openai**: OpenAI SDK
- **Groq SDK**: For Grok API access

## Important Notes

### Best Practices

**DO**:
- Use fast models (Flash, Nano) for simple tasks
- Use high-performance models for complex reasoning
- Set appropriate temperature for your use case
- Monitor token usage with `llmUsageTracker`
- Handle provider-specific errors gracefully

**DON'T**:
- Use GPT-5 with custom temperature/maxTokens (not supported)
- Forget to handle API key errors
- Assume all providers support all features
- Ignore rate limits

### Error Handling

```typescript
try {
  const response = await callLLM({...});
} catch (error) {
  if (error.message.includes('API_KEY not configured')) {
    // Handle missing API key
  } else if (error.message.includes('Invalid model')) {
    // Handle invalid model
  } else if (error.message.includes('rate limit')) {
    // Handle rate limiting
  }
}
```

### Usage Tracking

The service automatically tracks LLM usage:

```typescript
import { trackLLMUsage } from '../services/llm/llmUsageTracker';

await trackLLMUsage({
  provider: 'gemini',
  model: 'gemini-2.5-flash',
  promptTokens: 100,
  completionTokens: 50,
  totalTokens: 150,
  feature: 'character-autocomplete'
});
```

## Testing

### Test with Gemini (Fast)

```bash
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "gemini",
    "model": "gemini-2.5-flash-lite",
    "userPrompt": "Explain Docker in one sentence"
  }'
```

### Test with OpenAI GPT-5 (High Performance)

```bash
# GPT-5 doesn't accept temperature or maxTokens - use defaults
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-5",
    "systemPrompt": "You are a programming expert.",
    "userPrompt": "How to implement JWT auth in Node.js?"
  }'
```

### Test with Grok (High Performance with Web Search)

```bash
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "grok",
    "model": "grok-4",
    "systemPrompt": "You are a technical AI expert.",
    "userPrompt": "Explain transformer vs RNN architectures",
    "temperature": 0.8,
    "maxTokens": 2000
  }'
```

## Troubleshooting

**"API_KEY not configured"**
- Verify API key is in `.env`
- Check key format (starts with correct prefix)
- Restart backend after adding key

**"Invalid model for provider"**
- Check model name in `/api/v1/llm/models`
- Verify model is available for the provider
- Ensure provider is correct

**"Missing required fields"**
- Verify all required fields are present
- Check field names are spelled correctly
- Ensure JSON is valid

**Parameter not supported**
- Check if model supports the parameter
- GPT-5 doesn't support `temperature` or `maxTokens`
- Remove unsupported parameters

## See Also

- **Tool Calling System**: `backend/src/services/llm/tools/.docs.md`
- **Model Configuration**: `backend/src/data/llm-models.json`
- **LLM Routes**: `backend/src/routes/v1/llm.ts`
- **Usage Tracker**: `backend/src/services/llm/llmUsageTracker.ts`
- **Gemini Documentation**: https://ai.google.dev/docs
- **OpenAI Documentation**: https://platform.openai.com/docs
