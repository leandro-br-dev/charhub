# LLM API Documentation

Sistema de gerenciamento de LLMs com suporte a **Google Gemini**, **OpenAI** e **XAI Grok**.

## üîë Configura√ß√£o de API Keys

Adicione as chaves de API no arquivo `.env`:

```bash
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
GROK_API_KEY=your_grok_api_key_here
```

### Onde obter as chaves:

- **Google Gemini**: https://ai.google.dev/
- **OpenAI**: https://platform.openai.com/api-keys
- **XAI Grok**: https://console.x.ai/

### ‚ö†Ô∏è Formato das chaves:

- **Gemini**: Come√ßa com `AIza...`
- **OpenAI**: Come√ßa com `sk-proj-...` ou `sk-...`
- **Grok**: Come√ßa com `xai-...` (‚ö†Ô∏è N√£o confundir com chaves Anthropic que come√ßam com `sk-ant-`)

## üìã Endpoints Dispon√≠veis

### 1. Listar todos os modelos
```http
GET /api/v1/llm/models
```

**Resposta:**
```json
{
  "success": true,
  "data": {
    "gemini": {
      "name": "Google Gemini",
      "models": { ... }
    },
    "openai": {
      "name": "OpenAI",
      "models": { ... }
    },
    "grok": {
      "name": "XAI Grok",
      "models": { ... }
    }
  }
}
```

### 2. Listar modelos de um provedor espec√≠fico
```http
GET /api/v1/llm/models/:provider
```

**Exemplo:**
```bash
curl http://localhost/api/v1/llm/models/gemini
```

### 3. Gerar resposta com LLM
```http
POST /api/v1/llm/chat
Content-Type: application/json
```

**Body:**
```json
{
  "provider": "gemini",
  "model": "gemini-2.5-flash",
  "systemPrompt": "You are a helpful assistant.",
  "userPrompt": "What is the capital of France?",
  "temperature": 0.7,
  "maxTokens": 1000
}
```

**Resposta:**
```json
{
  "success": true,
  "data": {
    "provider": "gemini",
    "model": "gemini-2.5-flash",
    "content": "The capital of France is Paris.",
    "usage": {
      "promptTokens": 15,
      "completionTokens": 8,
      "totalTokens": 23
    }
  }
}
```

## üöÄ Exemplos de Teste

### Teste com Google Gemini (R√°pido)
```bash
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "gemini",
    "model": "gemini-2.5-flash-lite",
    "userPrompt": "Explique o que √© Docker em uma frase"
  }'
```

### Teste com OpenAI GPT-5 (Alto Desempenho)
```bash
# GPT-5 n√£o aceita par√¢metros temperature ou maxTokens - use apenas defaults
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-5",
    "systemPrompt": "Voc√™ √© um expert em programa√ß√£o.",
    "userPrompt": "Como implementar autentica√ß√£o JWT em Node.js?"
  }'
```

### Teste com OpenAI GPT-4.1 Nano (R√°pido - com temperature customizada)
```bash
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4.1-nano",
    "systemPrompt": "Voc√™ √© um expert em programa√ß√£o.",
    "userPrompt": "Explique o que √© REST API em uma frase",
    "temperature": 0.5
  }'
```

### Teste com XAI Grok 4 (Alto Desempenho)
```bash
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "grok",
    "model": "grok-4",
    "systemPrompt": "You are a technical advisor with deep knowledge of AI systems.",
    "userPrompt": "Explain the difference between transformer and RNN architectures",
    "temperature": 0.8,
    "maxTokens": 2000
  }'
```

## üìä Classifica√ß√£o de Modelos (Outubro 2025)

### ‚ö° **R√°pidos** (Fast)
Otimizados para velocidade e menor custo:
- `gemini-2.5-flash-lite` - Modelo flash mais r√°pido do Gemini
- `gpt-5-nano` - Variante mais eficiente do GPT-5
- `gpt-4.1-nano` - Mais r√°pido e barato (80.1% MMLU)
- `grok-4-fast-non-reasoning` - Grok 4 sem modo racioc√≠nio (2M context)

### üìà **M√©dio Desempenho** (Medium)
Balan√ßo entre qualidade e velocidade:
- `gemini-2.5-flash` - Melhor custo-benef√≠cio do Gemini
- `gemini-2.0-flash` - Features de nova gera√ß√£o
- `gpt-5-mini` - GPT-5 balanceado
- `grok-code-fast-1` - Especializado em c√≥digo

### üß† **Alto Desempenho** (High)
M√°xima qualidade de racioc√≠nio:
- `gemini-2.5-pro` - IA mais inteligente com pensamento adaptativo
- `gpt-5` - Melhor para c√≥digo e tarefas agentic (74.9% SWE-bench)
- `gpt-realtime` - Speech-to-speech avan√ßado
- `grok-4` - Modelo mais inteligente com busca em tempo real
- `grok-4-fast-reasoning` - Performance frontier com efici√™ncia

## üéØ Par√¢metros Dispon√≠veis

| Par√¢metro | Tipo | Obrigat√≥rio | Descri√ß√£o |
|-----------|------|-------------|-----------|
| `provider` | string | ‚úÖ | `gemini`, `openai` ou `grok` |
| `model` | string | ‚úÖ | Nome do modelo (veja `/api/v1/llm/models`) |
| `userPrompt` | string | ‚úÖ | Mensagem do usu√°rio |
| `systemPrompt` | string | ‚ùå | Instru√ß√µes do sistema |
| `temperature` | number | ‚ùå | 0.0 a 1.0 (padr√£o: 0.7) - **N√£o suportado em modelos GPT-5** |
| `maxTokens` | number | ‚ùå | Limite de tokens na resposta - **N√£o suportado em modelos GPT-5** |

### ‚ö†Ô∏è Limita√ß√µes por Modelo

**GPT-5 e GPT-Realtime:**
- N√£o aceitam par√¢metros `temperature`, `top_p` ou `maxTokens`
- Utilizam apenas valores padr√£o (temperature=1)
- Par√¢metros customizados foram removidos por serem modelos de racioc√≠nio

**Outros modelos (Gemini, Grok, GPT-4.1):**
- Suportam todos os par√¢metros normalmente

## ‚ö†Ô∏è Tratamento de Erros

### API Key n√£o configurada
```json
{
  "success": false,
  "error": "GEMINI_API_KEY not configured"
}
```

### Modelo inv√°lido
```json
{
  "success": false,
  "error": "Invalid model gpt-5 for provider openai"
}
```

### Campos obrigat√≥rios ausentes
```json
{
  "success": false,
  "error": "Missing required fields: provider, model, userPrompt"
}
```

## üîç Teste R√°pido no Terminal

```bash
# 1. Listar modelos dispon√≠veis
curl http://localhost/api/v1/llm/models

# 2. Testar resposta simples com o modelo mais r√°pido
curl -X POST http://localhost/api/v1/llm/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "gemini",
    "model": "gemini-2.5-flash-lite",
    "userPrompt": "Diga ol√° em 5 idiomas"
  }'
```

## üìù Pr√≥ximos Passos

Para usar a API, voc√™ precisa:

1. **Obter API Keys** dos provedores que deseja usar
2. **Adicionar no .env** do backend
3. **Rebuild do backend**: `docker-compose build backend`
4. **Testar os endpoints** conforme exemplos acima

## üõ†Ô∏è Estrutura de Arquivos

```
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm-models.json        # Configura√ß√£o dos modelos
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ index.ts           # Gerenciador principal
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gemini.ts          # Servi√ßo Google Gemini
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ openai.ts          # Servi√ßo OpenAI
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ grok.ts            # Servi√ßo XAI Grok
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îî‚îÄ‚îÄ v1/
‚îÇ           ‚îî‚îÄ‚îÄ llm.ts             # Rotas da API
```
